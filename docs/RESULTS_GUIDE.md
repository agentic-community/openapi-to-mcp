# Understanding Generated Results

This guide explains the files generated by the OpenAPI to MCP Converter and their purposes.

## Directory Structure

When you run the converter on an OpenAPI specification, it creates a results directory with the following structure:

```
results_YYYYMMDD_HHMMSS_<spec-name>_<provider>/
├── evaluation_YYYYMMDD_HHMMSS.json      # Detailed evaluation results
├── summary_YYYYMMDD_HHMMSS.md           # Human-readable summary report
├── usage_YYYYMMDD_HHMMSS.json           # Token usage and cost tracking
├── enhanced_spec_YYYYMMDD_HHMMSS.yaml   # AI-enhanced OpenAPI specification
├── original_spec_YYYYMMDD_HHMMSS.yaml   # Original specification (for reference)
└── mcpserver/                            # Generated MCP implementation
    ├── server.py                         # MCP server implementation
    ├── client.py                         # MCP client for testing
    ├── requirements.txt                  # Python dependencies
    ├── README.md                         # Usage instructions
    └── tool_spec.txt                     # Tool specifications
```

## File Descriptions

### Evaluation Files

#### `evaluation_YYYYMMDD_HHMMSS.json`
[View example: evaluation_20250718_150544.json](../examples/results/anthropic/sample_api/evaluation_20250718_150544.json)

This is the comprehensive evaluation output containing:
- **API metadata**: Title, version, and OpenAPI version
- **Overall scores**: Quality ratings, completeness score (1-5), and AI readiness score (1-5)
- **Operations analysis**: Detailed review of each endpoint including:
  - Description quality assessment
  - Parameter documentation completeness
  - Response documentation quality
  - Pagination and filtering support for list operations
  - Specific suggestions for improvement
- **Schema analysis**: Review of data models and their documentation
- **Security analysis**: Assessment of authentication and authorization schemes
- **Linting results**: OpenAPI specification compliance issues

#### `summary_YYYYMMDD_HHMMSS.md`
[View example: summary_20250718_150544.md](../examples/results/anthropic/sample_api/summary_20250718_150544.md)

A developer-friendly markdown report that presents the evaluation results in an easy-to-read format:
- **Overview section**: Basic API information and overall scores
- **Pagination/Filtering indicators**: Shows which endpoints support these features
- **Operations table**: Quick overview of all endpoints with quality indicators
- **Detailed analysis**: In-depth review of each operation with suggestions
- **Key strengths and improvements**: Actionable recommendations

#### `usage_YYYYMMDD_HHMMSS.json`
[View example: usage_20250718_150544.json](../examples/results/anthropic/sample_api/usage_20250718_150544.json)

Tracks the LLM usage and costs for the conversion process:
- Token counts (prompt and completion)
- Number of LLM calls made
- Estimated costs in USD
- Breakdown by operation (evaluation vs. generation)

### Specification Files

#### `enhanced_spec_YYYYMMDD_HHMMSS.yaml`
*Note: Enhanced spec not included in pre-generated example*

The AI-enhanced version of your OpenAPI specification with:
- Improved descriptions for operations and parameters
- Added examples where missing
- Enhanced schema documentation
- Better error response documentation
- Pagination/filtering parameters for list operations

#### `original_spec_YYYYMMDD_HHMMSS.yaml`
[View example: original_spec_20250718_150544.yaml](../examples/results/anthropic/sample_api/original_spec_20250718_150544.yaml)

A copy of the original specification for comparison and reference.

### Generated MCP Server

#### `mcpserver/server.py`
[View example: server.py](../examples/results/anthropic/sample_api/mcpserver/server.py)

The main MCP server implementation that:
- Implements each OpenAPI operation as an MCP tool
- Handles authentication using environment variables
- Makes HTTP requests to the actual API backend
- Provides proper error handling and response formatting
- Supports configuration via command-line arguments:
  - `--port`: MCP server listening port (default: 9001)
  - `--base-url`: Backend API URL (default: http://localhost:9002)

#### `mcpserver/client.py`
[View example: client.py](../examples/results/anthropic/sample_api/mcpserver/client.py)

A ready-to-use MCP client for testing that:
- Connects to the MCP server
- Lists all available tools
- Provides an interactive interface to test each operation
- Displays formatted responses

#### `mcpserver/requirements.txt`
[View example: requirements.txt](../examples/results/anthropic/sample_api/mcpserver/requirements.txt)

Lists all Python dependencies needed to run the MCP server:
- `mcp`: Model Context Protocol library
- `requests`: For HTTP requests to the backend API
- `httpx`: Alternative HTTP client
- `pydantic`: For data validation

#### `mcpserver/README.md`
[View example: README.md](../examples/results/anthropic/sample_api/mcpserver/README.md)

Contains:
- Quick start instructions
- Environment variable configuration
- Usage examples for each tool
- Troubleshooting tips

#### `mcpserver/tool_spec.txt`
[View example: tool_spec.txt](../examples/results/anthropic/sample_api/mcpserver/tool_spec.txt)

Detailed specifications for each generated MCP tool including:
- Tool name and description
- Required and optional parameters
- Parameter types and constraints
- Expected response format

## Quality Thresholds

The converter uses these thresholds to determine if MCP generation should proceed:
- **Completeness Score**: Must be ≥ 3/5
- **AI Readiness Score**: Must be ≥ 3/5

Both scores must meet the threshold for MCP server generation. If not met, the evaluation report will contain specific suggestions for improving your OpenAPI specification.

## Pre-Generated Sample Results

The `examples/results/anthropic/sample_api/` directory contains pre-generated results from evaluating the sample space mission API. This serves as:
- A reference for what good evaluation results look like
- A ready-to-run example for testing the stub server integration
- A demonstration of the complete conversion process

To explore these results:
1. Review the `summary_*.md` file for a high-level overview
2. Check the `evaluation_*.json` for detailed scoring
3. Run the pre-generated MCP server with the stub backend (see main README)